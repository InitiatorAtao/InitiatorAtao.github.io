<h1 id="o1-replication-journey-o1-复制之旅">o1 Replication Journey (o1
复制之旅)</h1>
<h2 id="初始评估">初始评估</h2>
<ul>
<li><p>o1 的思想</p>
<ul>
<li><p>检查 OpenAI 提供的 o1 思考示例.</p>
<p>回答长度 (包括标记和行数)
随难度成比例增长,难度越高的问题涉及的推理步骤越多.</p></li>
<li><p>进行关键词频率分析.</p>
<p>推理过程中的分支,反思和自我修正.</p></li>
<li><p>提取 o1 处理复杂方程并进行推理的基本思维链.</p>
<p>结论:长思维数据需要具有
"迭代解决问题","标记关键思维","递归和反思方法","探索假设","结论与验证"
的特征.</p></li>
</ul></li>
<li><p>"长思" 如何工作</p>
<p>目前尚未有足够的经验证据验证假设的准确性.</p>
<ul>
<li><p>旅程学习 (Journey Learning)</p>
<p>探索整个决策轨迹,考虑多种解决路径,从错误中学习,并理解完整的问题解决过程.</p>
<p>不仅仅是知道正确答案,还能理解为什么以及如何得出正确答案.</p></li>
</ul></li>
<li><p>如何构建长思</p>
<ul>
<li><p>使用 LLM 和奖励的树状搜索</p>
<p>构建推理树,根节点代表问题,其他节点代表推理步骤,路径代表推理过程.回溯和反思基于不正确的推理步骤.</p></li>
<li><p>建议-批判循环</p>
<p>允许模型选择当前行动来构建推理树.</p></li>
<li><p>多智能体 (Agent) 方法</p>
<p>一个智能体作为策略模型,不断进行推理,另一个智能体作为批评模型,指出应当继续还是回溯.</p></li>
<li><p>标注完整的人类思维过程</p>
<p>记录人类解决推理任务的过程作为长思.</p></li>
</ul></li>
<li><p>如何构建奖励模型</p>
<p>使用数据集 PRM800K 和 MR-GSM8K 对几个模型进行测试,o1-mini
的表现最好.</p></li>
<li><p>如何构建策略推理树</p>
<ul>
<li><p>策略模型和步骤分割</p>
<p>使用 Abel
中提出的格式,将答案分为多行,每行以行号开头,包括行内的推理</p>
<p>使用 Abel 的数据集对 DeepSeekMath-7B-Base 进行了微调,得到了
Abel-DSMath 作为策略模型.</p></li>
<li><p>奖励模型和剪枝</p>
<p>使用 o1-mini 直接显示每个推理步骤的正确与否,每次选择最多 <span
class="math inline">\(K\)</span> 个正确的推理步骤.</p></li>
</ul></li>
<li><p>如何从推理树中推导出长思想</p>
<ul>
<li><p>从推理树中构建捷径</p>
<p>即一条通向正确答案叶节点的路径,如果有多个则建立多条路径</p></li>
<li><p>推理树的遍历路径</p>
<p>深搜,引入特定的约束来限制搜索的复杂性,即在正确路径上可以进行最多
<span class="math inline">\(K\)</span>
次正确或错误的探索,在错误路径上随机选择下一步且回溯之后不再继续探索.</p></li>
<li><p>从遍历路径得出长思</p>
<p>串联遍历路径中的所有步骤来构建一个长思草稿,并使用 GPT-4o
修改草稿.</p></li>
</ul></li>
<li><p>如何评估实验</p>
<p>使用 Streamlit
搭建可视化数据分析平台,可视化推理树和长思以及模型输出.</p></li>
<li><p>如何训练模型</p>
<p>使用 DeepSeekMath-7B-Base 作为预训练模型,先后进行监督微调 (Supervised
Fine-Tuning,SFT) 和直接偏好学习 (Direct Preference
Optimization,DPO).</p>
<ul>
<li><p>监督微调</p>
<p>先后进行捷径学习 (Abel 数据集和筛选后的 PRM800K 数据集) 和旅程学习
(327 个示例,同时使用对应的捷径训练进行对比)</p></li>
<li><p>直接偏好学习</p>
<p>使用从 PRM800K 中重新划分的 MATH Train 数据集生成每个问题的 20
个回答,按照正确性分类,抽取 5 个正误偏好对训练模型.</p></li>
</ul></li>
<li><p>人类与人工智能合作的有效注释策略</p>
<p>需要记录"完整的思维过程"与"常识的补充解释",数据扩增的方法如下:</p>
<ul>
<li><p>增强数据粒度</p></li>
<li><p>渐进式推理</p></li>
<li><p>学生-探索者的视角</p></li>
</ul></li>
</ul>
<h2 id="工具一览">工具一览</h2>
<ul>
<li><p>PRM800K:在 <a
href="https://openreview.net/forum?id=v8L0pN6EOi">Let’s Verify Step by
Step</a> 发布的 800,000 个步骤级人类反馈标签的完整数据集.
在文中用于测试奖励模型的性能.</p></li>
<li><p>GSM8K: OpenAI 在 <a
href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math
Word Problems</a> 发布的 8.5K
语言多样化的小学数学单词问题的数据集.在文中用于测试奖励模型的性能.</p></li>
<li><p>Abel: 在 <a href="https://github.com/GAIR-NLP/abel">Generative AI
for Math: Abel</a> 发布的仅使用 SFT
构建的模型.在文中使用了其提出的数据格式和数据集.</p></li>
<li><p>DeepSeekMath:在 <a
href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits
of Mathematical Reasoning in Open Language Models</a> 发布的对 DeepSeek
模型进行预训练产生的模型.文中使用了其 7B-Base
版本作为策略模型基座.</p></li>
</ul>
