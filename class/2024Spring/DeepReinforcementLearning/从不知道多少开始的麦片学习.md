### 我是大麦片哥

#### 论文阅读

##### 背景信息

环境:长度可变的 episodic environment (有明确开始和结束的环境),形式化为马尔可夫决策过程

 $\mu $:初始 state 分布

 $\gamma \in [0,1)$: 衰减率

 $S_{0}\sim \mu $ :遵从分布 $\mu $ 的初始状态

 $t$ :时间步

 $\pi $ :决策

 $A_{t}\sim \pi (A_{t}|S_{t})$ :遵从 $\pi $ 分布的决策

 $R_{t+1}$ :进行决策 $A_{t}$ 后得到的奖励,转移到下一个 state $S_{t+1}$ 

$$\begin{align*}
q_{\pi }(s,a)=\mathbb{E}\left[\sum_{t=0}^{}\gamma ^{t}R_{t+1}|\pi ,S_{0}=s,A_{0}=a\right]\tag{1}
\end{align*}$$ 经过 state-action 对后的衰减 reward 和的期望,称作 action-value 或 Q-value.

 $v_{\pi }(s)=\mathbb{E}_{A\sim \pi (\cdot |s)}[q_{\pi }(s,A)]$ :一个 state 的价值,即遵从决策 $\pi $ 得到的 Q-value 期望

**目标**:找到一个策略 $\pi $ 使得最大化初始 state 分布下 state 价值的期望:$$\begin{align*}
J(\pi )=\mathbb{E}_{S\sim \mu }[v_{\pi }(S)]\tag2
\end{align*}$$

策略优化:给予一个策略 $\pi _{\text{prior}}$ 以及其 Q-values $q_{\pi _{\text{prior}}}(s,a)$ ,构建一个新的策略 $\pi $ 使得 $v_{\pi }(s)\ge v_{\pi _{\text{prior}}}(s)\forall s$ ,基本的步骤如构建贪婪策略:$$\begin{align*}
\argmax_{\pi }\mathbb{E}_{A\sim \pi (\cdot |s)}[q_{\pi _{\text{prior}}}(s,A)]\tag3
\end{align*}$$正规化策略优化:求解如下问题:$$\begin{align*}
\argmax_{\pi }(\mathbb{E}_{A\sim \pi (\cdot|s)}[\hat{q}_{\pi _{\text{prior}}}(s,A)]-\Omega (\pi ))
\end{align*}$$其中 $\hat{q}_{\pi _{\text{prior}}}(s,a)$ 是关于策略 $\pi _{\text{prior}}$ 拟合的 Q-values, $\Omega (\pi )\in \mathbb{R}$ 是正规化变量,如负熵 $\Omega (\pi )=-\lambda \Eta [\pi ],\lambda $ 是熵权重,或 $\Omega (\pi )=\lambda \operatorname{KL}(\pi _{\text{prior}},\pi )$ ,于 TRPO 中使用.

求解方法:直接或间接法,如果 $\pi (a|s)$ 相对于策略参数是可微分的,那么直接应用梯度上升法可得$$\begin{align*}
J(s,\pi )=\mathbb{E}_{A\sim \pi (\cdot |s)}[\hat{q}_{\pi _{\text{prior}}}(s,A)]-\Omega (\pi )
\end{align*}$$使用对数导数技巧对期望梯度进行采样的梯度取样,可以得到典型的 (正则化) 策略梯度更新

##### 算法过程

