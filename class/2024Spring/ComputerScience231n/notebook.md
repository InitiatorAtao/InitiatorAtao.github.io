#### Batch normalization

其实就是把层间数据分布变成正态分布的方法，令 $\mu$ 为均值，$\sigma^2$ 为方差，那么 $x'=\frac{x-\mu}{\sqrt{\sigma^2+\epsilon}}$ 按标准正态分布，需要非标准的正态分布就加线性变换 $y'=ax'+b$ 并用梯度学习参数 $a,b$。

注意均值和方差是训练数据统计出来的，测试时直接用，不用再计算了。（但可能导致 Bug？需要注意）如果是全连接层，就对整个输入向量求统计值，如果是卷积层就对每个通道求统计值。

#### 激活函数

Sigmoid $f(x)=\frac 1 {1+e^{-x}},f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}=f^2(x)(\frac 1{f(x)}-1)$

tanh $f(x)=\tanh(x),f'(x)=\frac 1{\cosh^2(x)}$

ReLU $f(x)=\max(0,x),f'(x)=[x>0]$

Leaky ReLU $f(x)=max(0.1x,x),f'(x)=0.1[x<0]+[x>0]$

Maxout 两个线性函数取最大，导数就是选取的那个线性函数的导数

ELU $f(x)=x[x\geq0]+\alpha(e^x-1)[x<0],f'(x)=[x\geq0]+\alpha e^x[x<0]$

#### 损失函数

softmax 损失：所有输出加指数变成正数，再换算成占比取负对数，乘上真实标签求和。

L1 L2 损失：就是对预测与现实的差绝对值求和或平方和

smooth L1 损失：$[-1,1]$ 间是 L2，其他地方是 L1

#### 前期工作

##### 初始化

权重初始化不好可能会导致激活图像出现在中间或两侧的峰值，导致梯度消失，学习失败。

对于 tanh 这样的奇激活函数，Xavier 初始化可以做出类似正态分布的效果，对于全连接层，初始化为一个均值为0的正态分布，方差设为 $\frac 2{n_{in}+n_{out}}$，对于卷积层方差变为 fliter 的参数个数（也就是长宽，通道数的乘积）的算术平方根

对于 ReLU 激活，方差需要额外乘 $2$，因为只截取一半，初始化的目标是让输入数据和输出数据具有接近的方差，激活截取掉的部分需要在权重上补全。

初始化的效果和正则化 normalization 是差不多的，都能保护数据在合理范围内。

##### 超参数选择

首先空跑一遍损失函数（随机输出），看损失是否符合统计规律。（例如 softmax 在 C 个平均随机分布的输出下应该是 $-\log(\frac1C)=\log(C)$）

然后对一个小样例跑过拟合，如果损失不下降就调高学习率，损失跑到 nan 就调低学习率。（在 1e-1 到 1e-4 间尝试一下）

#### 训练方法

##### 输出均值处理

训练后输出向所有输出节点输出同样的值收敛

##### Mini-batch SGD

随机梯度下降，随机选择一组数据，向前传输，计算损失，后向传输，以梯度更新参数。（最朴素了）

